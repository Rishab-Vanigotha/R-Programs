form <- paste(regressors, collapse = '+')
form_in <- formula(paste(val, '~', form))
vif_init<-rbind(vif_init, c(val, VIF(lm(form_in, data = in_frame, ...))))
}
vif_max<-max(as.numeric(vif_init[,2]), na.rm = TRUE)
if(vif_max < thresh){
if(trace==T){ #print output of each iteration
prmatrix(vif_init,collab=c('var','vif'),rowlab=rep('',nrow(vif_init)),quote=F)
cat('\n')
cat(paste('All variables have VIF < ', thresh,', max VIF ',round(vif_max,2), sep=''),'\n\n')
}
return(var_names)
}
else{
in_dat<-in_frame
#backwards selection of explanatory variables, stops when all VIF values are below 'thresh'
while(vif_max >= thresh){
vif_vals<-NULL
var_names <- names(in_dat)
for(val in var_names){
regressors <- var_names[-which(var_names == val)]
form <- paste(regressors, collapse = '+')
form_in <- formula(paste(val, '~', form))
vif_add<-VIF(lm(form_in, data = in_dat, ...))
vif_vals<-rbind(vif_vals,c(val,vif_add))
}
max_row<-which(vif_vals[,2] == max(as.numeric(vif_vals[,2]), na.rm = TRUE))[1]
vif_max<-as.numeric(vif_vals[max_row,2])
if(vif_max<thresh) break
if(trace==T){ #print output of each iteration
prmatrix(vif_vals,collab=c('var','vif'),rowlab=rep('',nrow(vif_vals)),quote=F)
cat('\n')
cat('removed: ',vif_vals[max_row,1],vif_max,'\n\n')
flush.console()
}
in_dat<-in_dat[,!names(in_dat) %in% vif_vals[max_row,1]]
}
return(names(in_dat))
}
}
y<-rand.vars %*% matrix(parms) + rnorm(num.obs,sd=20)
lm.dat<-data<-data.frame(y,rand.vars)
form.in<-paste('y~', paste(names(lm.dat)[-1], collapse='+'))
mod1<-lm(form.in,data=lm.dat)
summary(mod1)
vif_func(in_frame=rand.vars, thresh=5,trace=T)
keep.dat<-vif_func(in_frame=rand.vars, thresh=5, trace=F)
form.in<-paste('y~', paste(keep.dat, collapse='+'))
mod2<-lm(form.in,data=lm.dat)
summary(mod2)
library(TH.data)
library(caret)
data("GlaucomaM", package = "TH.data")
trainData <- GlaucomaM
View(trainData)
dim(trainData)
head(trainData)
str(trainData)
set.seed(100)
options(warn = -1)
subsets <- c(1:5, 10, 15, 18)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(x = trainData[, c(1:3, 5:13)], y=trainData$Class,
sizes = subsets,
rfeControl = ctrl)
lmProfile
input<-iris
names(input)
str(input)
model = prcomp(input[,1:4], scale=TRUE)
model$sdev
model$rotation
model$center
model$scale
par(mfrow=c(2,2))
plot(model$x[,1], col=input[,5])
plot(model$x[,2], col=input[,5])
plot(model$x[,3], col=input[,5])
plot(model$x[,4], col=input[,5])
model$sdev^2 / sum(model$sdev^2)
plot(model)
## PCA without &#39;prcomp&#39;
## Normalize the input feature.
input$sepal_len1 = (input$sepal_len - mean(input$sepal_len) )/sd(input$sepal_len)
input
head(input)
## PCA without &#39;prcomp&#39;
## Normalize the input feature.
input$sepal_len1 = (input$Sepal.Length - mean(input$Sepal.Length) )/sd(input$Sepal.Length)
input$sepal_wid1 = (input$Sepal.Width - mean(input$Sepal.Width))/sd(input$Sepal.Width)
input$petal_len1 = (input$Petal.Length - mean(input$Petal.Length))/sd(input$Petal.Length)
input$petal_wid1 = (input$Petal.Width - mean(input$Petal.Width))/sd(input$Petal.Width)
##Get the covarience matrix and eigen vector.
matrix_form = matrix(c(input$sepal_len1, input$sepal_wid1, input$petal_len1, input$petal_wid1),
ncol=4)
m = cov(matrix_form)
eigenV = eigen(m)
eigenV$vectors
library(TH.data)
library(caret)
data("GlaucomaM", package = "TH.data")
trainData <- GlaucomaM
#View(trainData)
dim(trainData)
head(trainData)
str(trainData)
set.seed(100)
options(warn = -1)
subsets <- c(1:5, 10, 15, 18)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(x = trainData[, c(1:3, 5:13)], y=trainData$Class,
sizes = subsets,
rfeControl = ctrl)
lmProfile
input<-iris
names(input)
str(input)
model = prcomp(input[,1:4], scale=TRUE)
model$sdev
model$rotation
model$center
model$scale
par(mfrow=c(2,2))
plot(model$x[,1], col=input[,5])
plot(model$x[,2], col=input[,5])
plot(model$x[,3], col=input[,5])
plot(model$x[,4], col=input[,5])
model$sdev^2 / sum(model$sdev^2)
plot(model)
## PCA without &#39;prcomp&#39;
## Normalize the input feature.
input$sepal_len1 = (input$Sepal.Length - mean(input$Sepal.Length) )/sd(input$Sepal.Length)
input$sepal_wid1 = (input$Sepal.Width - mean(input$Sepal.Width))/sd(input$Sepal.Width)
input$petal_len1 = (input$Petal.Length - mean(input$Petal.Length))/sd(input$Petal.Length)
input$petal_wid1 = (input$Petal.Width - mean(input$Petal.Width))/sd(input$Petal.Width)
##Get the covarience matrix and eigen vector.
matrix_form = matrix(c(input$sepal_len1, input$sepal_wid1, input$petal_len1, input$petal_wid1),
ncol=4)
m = cov(matrix_form)
eigenV = eigen(m)
eigenV$vectors
library("factoextra")
install.packages("factoextra")
library("factoextra")
data(decathlon2)
decathlon2.active <- decathlon2[1:23, 1:10]
decathlon2.active
head(decathlon2.active)
decathlon2.active.shape
head(decathlon2.active[, 1:6])
## Visualize eigenvalues (scree plot). Show the percentage of variances explained by
#each principal component.
fviz_eig(res.pca)
## Compute PCA
res.pca <- prcomp(decathlon2.active, scale = TRUE)
## Visualize eigenvalues (scree plot). Show the percentage of variances explained by
#each principal component.
fviz_eig(res.pca)
# Graph of individuals. Individuals with a similar profile are grouped together.
col.ind = “cos2”, # Color by the quality of representation
col.ind = "cos2", # Color by the quality of representation
# Graph of individuals. Individuals with a similar profile are grouped together.
Fviz_pca_ind(res.pca,
col.ind = "cos2", # Color by the quality of representation
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
# Graph of individuals. Individuals with a similar profile are grouped together.
fviz_pca_ind(res.pca,
col.ind = "cos2", # Color by the quality of representation
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
## Graph of variables. Positive correlated variables point to the same side of the plot.
Negative correlated variables point to opposite sides of the graph.
## Graph of variables. Positive correlated variables point to the same side of the plot.
## Negative correlated variables point to opposite sides of the graph.
fviz_pca_var(res.pca,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
## Biplot of individuals and variables
fviz_pca_biplot(res.pca, repel = TRUE,
col.var = "#2E9FDF", # Variables color
col.ind = "#696969" # Individuals color
)
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>%
createDataPartition(p = 0.8, list = FALSE)
training.samples <- Boston$medv %>% createDataPartition(p = 0.8, list = FALSE)
library(dplyr)
training.samples <- Boston$medv %>% createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
model1 <- lm(medv ~., data = train.data)
# Make predictions
predictions <- model1 %>% predict(test.data)
# Model performance
data.frame(
RMSE = RMSE(predictions, test.data$medv),
R2 = R2(predictions, test.data$medv)
)
vif(model1)
car::vif(model1)
max(car::vif(model1))
d<- data.frame(car::vif(model1))
d
max(car::vif(model1))
d[-max(car::vif(model1))]
d[-max(car::vif(model1)),]
d[~max(car::vif(model1)),]
d<- data.frame(car::vif(model1))
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>% createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
model1 <- lm(medv ~., data = train.data)
# Make predictions
predictions <- model1 %>% predict(test.data)
# Model performance
data.frame(
RMSE = RMSE(predictions, test.data$medv),
R2 = R2(predictions, test.data$medv)
)
car::vif(model1)
d
d<- data.frame(car::vif(model1), colnames<-c('x1','x2'))
d<- data.frame(car::vif(model1))
d
as.data.frame(lapply(d, function(x) x[-which.max(x)]))
library(MASS)
#data(package="MASS")
boston<-Boston
dim(boston)
names(boston)
#LOAD RANDOM FOREST PACKAGE
require(randomForest)
#SET SEED AND CREATE A SAMPLE TRAINING SET OF 300 OBSERVATIONS
set.seed(101)
train=sample(1:nrow(boston),300)
#LETS FIT THE RANDOM FOREST AND SEE HOW IT WORKS
rf.boston=randomForest(medv~.,data=boston,subset=train)
rf.boston
#LETS STORE THE MEAN SQUARE ERROR ON THE OBJECT (OUT OF BAG ERROR)
oob.err=double(13)
# TEST ERROR: MEAN SQUARE ERROR WHICH IS EQUALS TO MEAN((medv-pred)^2)
test.err=double(13)
test.err
{
fit=randomForest(medv~.,data=boston,subset=train,mtry=mtry,ntree=350)
oob.err[mtry]=fit$mse[350]
pred=predict(fit,boston[-train,])
test.err[mtry]=with(boston[-train,], mean((medv-pred)^2))
}
for(mtry in 1:13)
{
fit=randomForest(medv~.,data=boston,subset=train,mtry=mtry,ntree=350)
oob.err[mtry]=fit$mse[350]
pred=predict(fit,boston[-train,])
test.err[mtry]=with(boston[-train,], mean((medv-pred)^2))
}
#lets plot the
matplot(1:mtry, cbind(test.err,oob.err),pch=23,col=c("red","blue"),type="b",ylab="meansquared
Error")
legend("topright",legend=c("OOB","Test"),pch=23,col=c("red","blue"))
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>% createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
#lets plot the
matplot(1:mtry, cbind(test.err,oob.err),pch=23,col=c("red","blue"),type="b",ylab="meansquared
Error")
legend("topright",legend=c("OOB","Test"),pch=23,col=c("red","blue"))
#lets plot the
matplot(1:mtry, cbind(test.err,oob.err),pch=23,col=c("red","blue"),type="b",ylab="meansquared
Error")
legend("topright",legend=c("OOB","Test"),pch=23,col=c("red","blue"))
names(boston)
require(MASS)
require(clusterGeneration)
set.seed(2)
num.vars<-15
num.obs<-200
cov.mat<-genPositiveDefMat(num.vars, covMethod="unifcorrmat")$Sigma
cov.mat
dim(cov.mat)
rand.vars<-mvrnorm(num.obs,rep(0,num.vars),Sigma=cov.mat)
r
rand.vars
dim(rand.vars)
parms<-runif(num.vars,-10,10)
parms
y<-rand.vars %*% matrix(parms) + rnorm(num.obs,sd=20)
y
lm.dat<-data<-data.frame(y,rand.vars)
dim(lm.dat)
form.in<-paste('y~', paste(names(lm.dat)[-1], collapse='+'))
form.in
mod1<-lm(form.in,data=lm.dat)
mod1
vif_func(in_frame=rand.vars, thresh=5,trace=T)
form_in <- formula(paste(val, '~', form))
vif_func<-function(in_frame,thresh=10,trace=T,...){
library(fmsb)
if(any(!'data.frame' %in% class(in_frame))) in_frame<-data.frame(in_frame)
#get initial vif value for all comparisons of variables
vif_init<-NULL
var_names <- names(in_frame)
for(val in var_names){
regressors <- var_names[-which(var_names == val)]
form <- paste(regressors, collapse = '+')
form_in <- formula(paste(val, '~', form))
vif_init<-rbind(vif_init, c(val, VIF(lm(form_in, data = in_frame, ...))))
}
vif_max<-max(as.numeric(vif_init[,2]), na.rm = TRUE)
if(vif_max < thresh){
if(trace==T){ #print output of each iteration
prmatrix(vif_init,collab=c('var','vif'),rowlab=rep('',nrow(vif_init)),quote=F)
cat('\n')
cat(paste('All variables have VIF < ', thresh,', max VIF ',round(vif_max,2), sep=''),'\n\n')
}
return(var_names)
}
else{
in_dat<-in_frame
#backwards selection of explanatory variables, stops when all VIF values are below 'thresh'
while(vif_max >= thresh){
vif_vals<-NULL
var_names <- names(in_dat)
for(val in var_names){
regressors <- var_names[-which(var_names == val)]
form <- paste(regressors, collapse = '+')
form_in <- formula(paste(val, '~', form))
vif_add<-VIF(lm(form_in, data = in_dat, ...))
vif_vals<-rbind(vif_vals,c(val,vif_add))
}
max_row<-which(vif_vals[,2] == max(as.numeric(vif_vals[,2]), na.rm = TRUE))[1]
vif_max<-as.numeric(vif_vals[max_row,2])
if(vif_max<thresh) break
if(trace==T){ #print output of each iteration
prmatrix(vif_vals,collab=c('var','vif'),rowlab=rep('',nrow(vif_vals)),quote=F)
cat('\n')
cat('removed: ',vif_vals[max_row,1],vif_max,'\n\n')
flush.console()
}
in_dat<-in_dat[,!names(in_dat) %in% vif_vals[max_row,1]]
}
return(names(in_dat))
}
}
y<-rand.vars %*% matrix(parms) + rnorm(num.obs,sd=20)
lm.dat<-data<-data.frame(y,rand.vars)
form.in<-paste('y~', paste(names(lm.dat)[-1], collapse='+'))
mod1<-lm(form.in,data=lm.dat)
summary(mod1)
vif_func(in_frame=rand.vars, thresh=5,trace=T)
keep.dat<-vif_func(in_frame=rand.vars, thresh=5, trace=F)
keep.dat
library(TH.data)
library(caret)
data("GlaucomaM", package = "TH.data")
trainData <- GlaucomaM
#View(trainData)
dim(trainData)
head(trainData)
str(trainData)
set.seed(100)
options(warn = -1)
subsets <- c(1:5, 10, 15, 18)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(x = trainData[, c(1:3, 5:13)], y=trainData$Class,
sizes = subsets,
rfeControl = ctrl)
lmProfile
library(psych)
fit <- principal(mtcars, nfactors=5, rotate='varimax')
fit # print results
fit <- factanal(mtcars, 3, rotation='varimax')
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2
load <- fit$loadings[,1:2]
plot(load,type='n') # set up plot
text(load,labels=names(mtcars),cex=.7) # add variable names
# Principal Axis Factor Analysis
library(psych)
fit <- factor.pa(mtcars, nfactors=3, rotation='varimax')
fit # print results
library(nFactors)
ev <- eigen(cor(mtcars)) # get eigenvalues
ap <- parallel(subject=nrow(mtcars),var=ncol(mtcars),
rep=100,cent=.05)
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(nS)
library(tidyverse)
#use caret package for machine learning workflow
library(caret)
theme_set(theme_bw())
set.seed(123)
training.samples <- marketing$age %>% createDataPartition(p = 0.8, list = FALSE)
RMSE(predictions, test.data$age)
data('marketing')
install.packages("datarium")
library(datarium)
data('marketing')
training.samples <- marketing$age %>% createDataPartition(p = 0.8, list = FALSE)
marketing
training.samples <- marketing$sales %>% createDataPartition(p = 0.8, list = FALSE)
train.data <- marketing[training.samples,]
test.data <- marketing[-training.samples,]
train.data.head()
train.data.head
head(train.data)
len(train.data)
length(train.data)
model <- lm(sales ~., data = train.data)
summary(model)
predictions <- model %>% predict(test.data)
RMSE(predictions, test.data$age)
R2(predictions, test.data$age)
RMSE(predictions, test.data$sales)
R2(predictions, test.data$sales)
library(tidyverse)
#use caret package for machine learning workflow
library(caret)
theme_set(theme_bw())
set.seed(123)
data('marketing')
training.samples <- marketing$sales %>% createDataPartition(p = 0.8, list = FALSE)
train.data <- marketing[training.samples,]
test.data <- marketing[-training.samples,]
model <- lm(sales ~., data = train.data)
summary(model)
predictions <- model %>% predict(test.data)
RMSE(predictions, test.data$sales)
R2(predictions, test.data$sales)
install.packages("readxl")
library(readxl)
ageandheight <- read_excel('ageandheight.xls', sheet = 'Hoja2')
ageandheight <- read_excel('ageandheight.xls', sheet = 'Hoja2')
#Upload the data
lmHeight = lm(height ~ age, data = 'ageandheight.xls')
#Upload the data
lmHeight = lm(height ~ age, data = 'ageandheight.xls')
ageandheight <- read_excel('ageandheight.xls', sheet = 'Hoja2')
setwd("~/7th sem/DRMV_R")
ageandheight <- read_excel('ageandheight.xls', sheet = 'Hoja2')
#Upload the data
lmHeight = lm(height ~ age, data = 'ageandheight.xls')
head(ageandheight)
library(readxl)
ageandheight <- read_excel('ageandheight.xls')
library(readxl)
ageandheight <- read_excel('ageandheight.xls')
#Upload the data
lmHeight = lm(height ~ age, data = 'ageandheight.xls')
#Upload the data
lmHeight = lm(height~age, data = 'ageandheight.xls')
library(readxl)
ageandheight <- read_excel('ageandheight.xls')
#Upload the data
lmHeight = lm(height~age, data = 'ageandheight')
typeof(ageandheight)
ageandheight <- data.frame(ageandheight)
head(ageandheight)
library(readxl)
ageandheight <- read_excel('ageandheight.xls')
ageandheight <- data.frame(ageandheight)
#Upload the data
lmHeight = lm(height~age, data = 'ageandheight')
res.ftest <- var.test(height~age, data = 'geandheight')
res.ftest <- var.test(height~age, data = 'ageandheight')
#Upload the data
lmHeight = lm(height~age, data = 'ageandheight')
ageandheight <- read_excel('ageandheight.xls')
ageandheight <- data.frame(ageandheight)
#Upload the data
lmHeight = lm(height~age, data = ageandheight)
#Create the linear regression
summary(lmHeight)
res.ftest <- var.test(height~age, data = ageandheight)
res.ftest <- var.test(lmHeight, data = ageandheight)
head(ageandheight)
#Review the results
# sum of the squares residuals
x <- c(1,2,3,4,5)
y <- c(10,20,30,40,50)
t_line=lm(y ~ x)
residuals(t_line)
residuals(t_line)^2
sum(residuals(t_line)^2)
x <- c(10,20,30,40,50)
y <- c(20,24,40,45,90)
res.ftest <- var.test(y,x)
res.ftest
lmMod <- lm(dist ~ speed, data=cars)
par(mfrow=c(2,2))
plot(lmMod)
